# Laborat√≥rio de Prompt Injection e Defesa em LLMs

Este reposit√≥rio serve como um laborat√≥rio pr√°tico e um material de apoio para o estudo de vulnerabilidades em Grandes Modelos de Linguagem (LLMs). Atrav√©s de um Jupyter Notebook, √© demonstrado de forma clara e objetiva como ataques de **Prompt Injection** podem ocorrer e, mais importante, como implementar t√©cnicas de defesa eficazes.

O c√≥digo utiliza a plataforma [Ollama](https://ollama.com/) para rodar um modelo open-source localmente, permitindo que qualquer pessoa possa replicar os experimentos de forma segura e gratuita.

## üéØ Conceitos Demonstrados

O notebook explora os seguintes cen√°rios:

1.  **Assistente Vulner√°vel:** Uma implementa√ß√£o base de um chatbot que, apesar de ter um prompt de sistema bem definido, √© vulner√°vel a ataques.
2.  **Ataque de Hijacking com Tokens Especiais:** Demonstra√ß√£o de como um atacante, conhecendo os tokens de formata√ß√£o do modelo (`<|end_of_text|>`, etc.), pode injet√°-los para sequestrar o objetivo do LLM.
3.  **Ataque Sem√¢ntico (Falho):** Uma demonstra√ß√£o de que, para este modelo e configura√ß√£o, um ataque sem√¢ntico simples (sem tokens especiais) n√£o √© eficaz, ressaltando a import√¢ncia dos tokens na manipula√ß√£o.
4.  **Defesa 1: Sanitiza√ß√£o de Entrada:** Implementa√ß√£o de uma fun√ß√£o que remove os tokens especiais da entrada do usu√°rio, neutralizando o ataque de hijacking.
5.  **Defesa 2: Spotlighting (Delimita√ß√£o):** Implementa√ß√£o de uma abordagem de prompt mais robusta, que cria uma "caixa de seguran√ßa" ao redor da entrada do usu√°rio e instrui o modelo a n√£o tratar seu conte√∫do como comandos.

## ‚öôÔ∏è Pr√©-requisitos

Antes de executar o notebook, garanta que voc√™ tenha o seguinte ambiente configurado:

1.  **Python 3.10+**
2.  **Ollama instalado:** Siga as instru√ß√µes de instala√ß√£o em [ollama.com](https://ollama.com/).
3.  **Modelo Granite-3B baixado:** Ap√≥s instalar o Ollama, execute o seguinte comando no seu terminal para baixar o modelo que usamos nos exemplos:
    ```sh
    ollama pull granite-code:3b
    ```
    *(Nota: Embora o c√≥digo original use `granite3.3:2b`, o `granite-code:3b` √© uma alternativa comum e compat√≠vel. Ajuste o nome do modelo no notebook se necess√°rio.)*

## üìö Refer√™ncias e Leitura Adicional

As t√©cnicas e vulnerabilidades demonstradas neste reposit√≥rio s√£o baseadas em pesquisas e padr√µes de seguran√ßa estabelecidos pela comunidade de IA.

1.  **[OWASP Top 10 for LLM Applications](https://owasp.org/www-project-top-10-for-large-language-model-applications/)**: A principal refer√™ncia da ind√∫stria para seguran√ßa em IA. O **Prompt Injection (LLM01)** √© listado como a vulnerabilidade n√∫mero um, validando a import√¢ncia dos experimentos neste notebook.

2.  **[Defending Against Indirect Prompt Injection Attacks With Spotlighting](https://ceur-ws.org/Vol-3920/paper03.pdf)**: Artigo de pesquisa que descreve formalmente a t√©cnica de **Spotlighting** (usando delimitadores e marca√ß√µes) como uma defesa robusta, explicando o princ√≠pio de separar dados de instru√ß√µes. Esta √© a base te√≥rica para a fun√ß√£o `chat_assistant_with_spotlighting`.

3.  **[Anthropic's Guide to Prompt Engineering](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/claude-4-best-practices)**: A documenta√ß√£o da Anthropic, embora focada no modelo Claude, oferece excelentes pr√°ticas que s√£o universalmente aplic√°veis. A recomenda√ß√£o de usar tags XML para separar o prompt √© um exemplo pr√°tico de Spotlighting.

4.  **[The Sandboxed Mind: Principled Isolation Patterns for Prompt-Injection-Resilient LLM Agents](https://arxiv.org/html/2505.13076v1)**: Um paper que discute padr√µes de arquitetura de sistema para isolar entradas n√£o confi√°veis, tratando a **sanitiza√ß√£o de entrada** e o encapsulamento como uma das camadas de defesa fundamentais.

Criado com üß† por **Alex Bispo**.

